<html>
<head>
	<style>
		.code {
			margin-left: 30px;
			color:#000000;
			background-color:#ffffff;
		}
	</style>
</head>
<body bgcolor=#d0d0a0><br><br><br><br>
<table align=center cellpadding=50 border=1 bgcolor=#e0e0d0 width=1000><tr><td>
<a href="../index.html#toc">Back to the table of contents</a><br>

<br>
<a href="bayesnet.html">Previous</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="bugs.html">Next</a>







<h2>Neural network examples</h2>
<p>This document gives examples for writing code to use the neural network class in Waffles.</p>


<br><br>
<h3>Logistic regression</h3>

<p>Logistic regression is fitting your data with a single layer of logistic units. Here are the #includes that we are going to need for this example:</p>

<pre class="code">
	#include &lt;GClasses/GActivation.h&gt;
	#include &lt;GClasses/GHolders.h&gt;
	#include &lt;GClasses/GMatrix.h&gt;
	#include &lt;GClasses/GNeuralNet.h&gt;
	#include &lt;GClasses/GRand.h&gt;
	#include &lt;GClasses/GTransform.h&gt;
</pre>

<p>We are going to need some data, so let's load some data from an ARFF file. Let's use <a href="http://mldata.org/repository/data/download/arff/datasets-uci-iris/">Iris</a>,
a well-known dataset for machine learning examples:</p>

<pre class="code">
	GMatrix data;
	data.loadArff("iris.arff");
</pre>

<p>"data" is a 150x5 matrix. Next, we need to divide this data into a feature matrix (the inputs) and a label matrix (the outputs):</p>

<pre class="code">
	GDataColSplitter cs(data, 1); // the "iris" dataset has only 1 column of "labels"
	const GMatrix&amp; inputs = cs.features();
	const GMatrix&amp; outputs = cs.labels();
</pre>

<p>"inputs" is a 150x4 matrix of real values, and "outputs" is a 150x1 matrix of categorical values.
Neural networks typically only support continuous values, but the labels in the iris dataset are categorical, so we will convert them to use a real-valued representation
(also known as a categorical distribution, a one-hot representation, or binarized representation):</p>

<pre class="code">
	GNominalToCat nc;
	nc.train(outputs);
	GMatrix* pRealOutputs = nc.transformBatch(outputs);
</pre>

<p>pRealOutputs points to a 150x3 matrix of real values. Now, lets further divide our data into a training portion and a testing portion:</p>

<pre class="code">
	GRand r(0);
	GDataRowSplitter rs(inputs, *pRealOutputs, r, 75);
	const GMatrix&amp; trainingFeatures = rs.features1();
	const GMatrix&amp; trainingLabels = rs.labels1();
	const GMatrix&amp; testingFeatures = rs.features2();
	const GMatrix&amp; testingLabels = rs.labels2();
</pre>

Now, we are ready to train a layer of logistic units that takes 4 inputs and gives 3 outputs. In this example, we will also specify the learning rate:</p>

<pre class="code">
	GNeuralNet nn;
	nn.addLayer(new GLayerClassic(4, 3, new GActivationLogistic()));
	nn.setLearningRate(0.05);
	nn.train(trainingFeatures, trainingLabels);
</pre>

<p>Let's test our model to see how well it performs:</p>

<pre class="code">
	double sse = nn.sumSquaredError(testingFeatures, testingLabels);
	double mse = sse / testingLabels.rows();
	double rmse = sqrt(mse);
	std::cout &lt;&lt; "The root-mean-squared test error is " &lt;&lt; to_str(rmse) &lt;&lt; "\n";
</pre>

<p>Finally, don't forget to delete pRealOutputs:</p>

<pre class="code">
	delete(pRealOutputs);
</pre>

<p>Or, preferably:</p>

<pre class="code">
	Holder&lt;GMatrix&gt; hRealOutputs(pRealOutputs);
</pre>




<br><br>
<h3>Classification</h3>

<p>(This example builds on the previous one.)</p>

<p>The previous example was not actually very useful because root-mean-squared only tells us how poorly the neural network fit to our
continuous representation of the data. It does not really tell us how accurate the neural network is for classifying this data.
So, instead of transforming the data to meet the model, we can transform the model to meet the data. Specifically,
we can use the GAutoFilter class to turn the neural network into a classifier:</p>

<pre class="code">
	GAutoFilter af(&amp;nn, false); // false means the auto-filter does not need to "delete(&amp;nn)" when it is destroyed.
</pre>

<p>Now, we can train the auto-filter using the original data (with nominal labels). We no longer need to explicitly train
the neural network because when we train the auto-filter, it trains the inner model.</p>

<pre class="code">
	af.train(inputs, outputs);
</pre>

<p>The auto-filter automatically filters the data as needed for its inner model, but ultimately behaves as a model that can
handle whatever types of data you have available. In this case, it turns a neural network into a classifier, since "outputs"
contains 1 column of categorical values. So, now we can obtain the misclassification rate as follows:</p>

<pre class="code">
	double mis = af.sumSquaredError(inputs, outputs);
	std::cout &lt;&lt; "The model misclassified " &lt;&lt; to_str(mis)  &lt;&lt;
		" out of " &lt;&lt; to_str(outputs.rows()) &lt;&lt; " instances.\n";
</pre>

<p>Why does a method named "sumSquaredError" return the total number of misclassifications? Because it uses Hamming distance for
categorical labels, which reports a squared-error of 1 for each misclassification.</p>

<p>(Note that if you are training a big network with big data, then efficiency may be critical. In such cases, it is generally better
to use the approach of transforming the data to meet the model, so that it does not waste a lot of time transformaing data during
training.)</p>



<br><br>
<h3>Adding layers</h3>

<p>To make a deeper neural network, you add more layers prior to training. For example:</p>

<pre class="code">
	nn.addLayer(new GLayerClassic(784, 1000));
	nn.addLayer(new GLayerClassic(1000, 300));
	nn.addLayer(new GLayerClassic(300, 90));
	nn.addLayer(new GLayerClassic(90, 10));
</pre>

<p>The layers are added in feed-forward order. The first layer added is the input layer. The last layer added is the output layer.
Notice how the number of inputs into each layer should be the same as the number of outputs in the previous layer.
If they do not line up, then the existing layer will be resized to fit with the newly added layer. In other words, if
there is a conflict, the last one added to the neural network wins. Alternatively, you can also use the constant "FLEXIBLE_SIZE" to
indicate that you want the adjacent layer to determine the size. For example, following code results in the very same topology:</p>

<pre class="code">
	nn.addLayer(new GLayerClassic(784, 1000));
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 300));
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 90));
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 10));
</pre>

<p>When training begins, the number of inputs and outputs will also be resized to fit the training data. So, you can
use FLEXIBLE_SIZE for those too:</p>

<pre class="code">
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 1000));
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 300));
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 90));
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, FLEXIBLE_SIZE));
</pre>

<p>That is as far as you can use FLEXIBLE_SIZE. If you use FLEXIBLE_SIZE anywhere else, then the size of one of the layers would be
ambiguous, which would be problematic.</p>



<br><br>
<h3>Activation functions</h3>

<p>If no activation function is specified, then the layer uses a default activation function. The default activation function for
GLayerClassic is GActivationTanH, which implements the tanh activation function. Many other activation functions are also available:</p>

<pre class="code">
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 1000, new GActivationRectifiedLinear()));
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 300, new GActivationSoftPlus()));
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 90, new GActivationGaussian()));
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, FLEXIBLE_SIZE, new GActivationIdentity()));
</pre>

<p>You can use the GLayerMixed class to make layers that contain a mixture of activation functions. In the following example,
the second layer has a size of 300, but it uses the softplus activation function for 150 of its units, and a sinusoid
activation function for the other 150 units:</p>

<pre class="code">
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 1000, new GActivationRectifiedLinear()));
	GLayerMixed* pMix = new GLayerMixed();
	pMix->addComponent(new GLayerClassic(1000, 150, new GActivationSoftPlus()));
	pMix->addComponent(new GLayerClassic(1000, 150, new GActivationSin()));
	nn.addLayer(new GLayerClassic(300, 90, new GActivationSinc()));
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, FLEXIBLE_SIZE, new GActivationBentIdentity()));
</pre>



<br><br>
<h3>Layer types</h3>

<p>There are several types of layers that you can use in your neural networks:
<ul>
	<li><b>GLayerClassic</b> - A traditional fully-connected feed-forward layer.</li>
	<li><b>GLayerMixed</b> - Allows you to combine multiple layers of different types into a single layer.</li>
	<li><b>GLayerRestrictedBoltzmannMachine</b>: A restricted boltzmann machine layer.</li>
	<li><b>GLayerConvolutional1D</b> - A 1-dimensional convolutional layer.</li>
	<li><b>GLayerConvolutional2D</b> - A 2-dimensional convolutional layer.</li>
	<li><b>GLayerSoftMax</b> - A soft-max layer, trained with cross-entropy.</li>
	<li><b>GLayerClassicCuda</b> - A GPU-optimized classic layer. Requires linking with the GCuda library.</li>
	<li>(Other layer types are currently under development...)</li>
</ul>



<br><br>
<h3>Stopping criteria</h3>

<p>The GNeuralNet::train method divides the training data into a training portion and a validation
portion. The default uses 65% for training and 35% for validation. Suppose you wanted to change this
ratio to 60/40. This would be done as follows:</p>

<pre class="code">
	nn.setValidationPortion(0.4);
</pre>

<p>By default, training continues until validation accuracy does not improve by 0.2% over a window
of 100 epochs. If you wanted to change this to 0.1% over a window of 10 epochs, then you could do:

<pre class="code">
	nn.setImprovementThresh(0.001);
	nn.setWindowSize(10);
</pre>




<br><br>
<h3>Training incrementally</h3>

<p>Sometimes, it is preferable to train your neural network incrementally, instead of simply calling the "train" method.
For example, you might want to use a custom stopping criteria, you might want to report validation accuracy before
each training epoch, you might want to decay the learning rate in a particular manner, etc. The following example
shows how such things can be implemented:

<pre class="code">
	nn.beginIncrementalLearning(trainingFeatures.relation(), trainingLabels.relation());
	GRandomIndexIterator ii(trainingFeatures.rows(), nn.rand());
	for(size_t epoch = 0; epoch &lt; total_epochs; epoch++)
	{
		// Report validation accuracy
		double rmse = sqrt(nn1.sumSquaredError(validateFeatures, validateLabels) / validateLabels.rows());
		std::cout &lt;&lt; to_str(rmse) &lt;&lt; "\n";
		std::cout.flush();
	
		// Train
		ii.reset();
		size_t index;
		while(ii.next(index))
		{
			nn.trainIncremental(trainingFeatures[index], trainingLabels[index]);
		}
	
		// Decay the learning rate
		nn.setLearningRate(nn.learningRate() * 0.98);
	}
</pre>





<br><br>
<h3>Serialization</h3>

<p>You can write your neural network to a file:</p>

<pre class="code">
GDom doc;
doc.setRoot(nn.serialize(&amp;doc));
doc.saveJson("my_neural_net.json");
</pre>

<p>Then, you can load it from the file again:</p>

<pre class="code">
GDom doc;
doc.loadJson("my_neural_net.json");
GLearnerLoader ll;
GNeuralNet* pNN = (GNeuralNet*)ll.loadLearner(doc.root());
</pre>


<br><br>
<h3>GPU Parallelization</h3>

<p>Suppose you want to train your neural network in parallel on a GPU. For
class neural network layers, this is done by simply replacing GLayerClassic with GLayerClassicCuda.</p>

<p>For example, if you make your neural network like this,</p>

<pre class="code">
	GNeuralNet nn;
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 2000));
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 5000));
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 1000));
</pre>

<p>then all you need to do to parallelize it is change these lines to:</p>

<pre class="code">
	GCudaEngine e;
	GNeuralNet nn;
	nn.addLayer(new GLayerClassicCuda(e, FLEXIBLE_SIZE, 2000));
	nn.addLayer(new GLayerClassicCuda(e, FLEXIBLE_SIZE, 5000));
	nn.addLayer(new GLayerClassicCuda(e, FLEXIBLE_SIZE, 1000));
</pre>

<p>The GLayerClassicCuda class is not in the GClasses library. It is in the
GCuda library. To build this library, go into "src/depends/GCuda" and do
"sudo make install". (Subfolders of "depends" are not built by default
because they depend on things that some users may not have installed.)
Of course, you also need to have an NVIDIA GPGPU, and you have to link to a few
extra libraries: GCuda, curand, cublas, and cudart. The latter three
can all be obtained by installing the Cuda SDK. To help you get started,
there is a demo app in demos/cuda, which shows how this is done.</p>

<p>Currently only classic layers have a GPU-counterpart, but I am working
on parallelizig the other layer types too. I plan to have all functionality
available on the GPU ...eventually. (If you would like to help with this
effort, I would very much appreciate it.)</p>




<br><br>
<h3>General-purpose code</h3>

<p>Here is some code for training a neural network on some arbitrary data.
This code may be a good starting point for many problems involving neural networks:</p>

<pre class="code">
#include &lt;iostream&gt;
#include &lt;cmath&gt;
#include &lt;GClasses/GApp.h&gt;
#include &lt;GClasses/GError.h&gt;
#include &lt;GClasses/GNeuralNet.h&gt;
#include &lt;GClasses/GActivation.h&gt;
#include &lt;GClasses/GTransform.h&gt;
#include &lt;GClasses/GVec.h&gt;
#include &lt;GClasses/GHolders.h&gt;

using namespace GClasses;
using std::cerr;
using std::cout;

int main(int argc, char *argv[])
{
	// Load and prep the training data
	GRand rand(0);
	GMatrix raw;
	raw.loadArff("/somepath/somedata.arff");
	GDataColSplitter s1(raw, 1);
	GDataRowSplitter s2(s1.features(), s1.labels(), rand, s1.labels().rows() / 2);

	// Make the network
	GNeuralNet nn;
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 80));
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 30));
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, FLEXIBLE_SIZE));
	nn.setLearningRate(0.01);
	GAutoFilter af(&amp;nn, false);
	af.beginIncrementalLearning(s2.features1(), s2.labels1());

	// Prep the data
	af.prefilterData(&amp;s2.features1(), &amp;s2.labels1(), &amp;s2.features2(), &amp;s2.labels2());
	GMatrix&amp; trainFeatures = *af.data()[0];
	GMatrix&amp; trainLabels = *af.data()[1];
	GMatrix&amp; testFeatures = *af.data()[2];
	GMatrix&amp; testLabels = *af.data()[3];

	// Print some stats
	cout &lt;&lt; "% Training patterns: " &lt;&lt; to_str(trainFeatures.rows()) &lt;&lt; "\n";
	cout &lt;&lt; "% Testing patterns: " &lt;&lt; to_str(testFeatures.rows()) &lt;&lt; "\n";
	cout &lt;&lt; "% Topology: " &lt;&lt; to_str(nn.layer(0).inputs());
	for(size_t i = 0; i &lt; nn.layerCount(); i++)
		cout &lt;&lt; " -&gt; " + to_str(nn.layer(i).outputs());
	cout &lt;&lt; "\n";
	cout &lt;&lt; "% Total weights: " &lt;&lt; to_str(nn.countWeights()) &lt;&lt; "\n";

	// Train
	cout &lt;&lt; "@RELATION neural_net_training\n";
	cout &lt;&lt; "@ATTRIBUTE internal_rmse_train real\n";
	cout &lt;&lt; "@ATTRIBUTE internal_rmse_test real\n";
	cout &lt;&lt; "@ATTRIBUTE misclassification_rate real\n";
	cout &lt;&lt; "@DATA\n";
	GRandomIndexIterator ii(trainFeatures.rows(), nn.rand());
	for(size_t epoch = 0; epoch &lt; 50000; epoch++)
	{
		// Validate
		double sseTrain = nn.sumSquaredError(trainFeatures, trainLabels);
		cout &lt;&lt; to_str(std::sqrt(sseTrain / trainFeatures.rows())) &lt;&lt; ", ";
		double sseTest = nn.sumSquaredError(testFeatures, testLabels);
		cout &lt;&lt; to_str(std::sqrt(sseTest / testFeatures.rows())) &lt;&lt; ", ";
		double mis = af.sumSquaredError(s2.features2(), s2.labels2());
		cout &lt;&lt; to_str((double)mis / s2.features2().rows()) &lt;&lt; "\n";
		cout.flush();

		// Do an epoch of training
		ii.reset();
		size_t index;
		while(ii.next(index))
			nn.trainIncremental(trainFeatures[index], trainLabels[index]);
	}
	return 0;
}
</pre>





<br><br>
<h3>MNIST</h3>

<p>A popular test for a neural network is the <a href="http://uaf46365.ddns.uark.edu/data/mnist/">MNIST dataset</a>.
So, let's do an example with it. Here is some code:</p>

<pre class="code">
#include &lt;iostream&gt;
#include &lt;cmath&gt;
#include &lt;GClasses/GApp.h&gt;
#include &lt;GClasses/GError.h&gt;
#include &lt;GClasses/GNeuralNet.h&gt;
#include &lt;GClasses/GActivation.h&gt;
#include &lt;GClasses/GTransform.h&gt;
#include &lt;GClasses/GVec.h&gt;
#include &lt;GClasses/GHolders.h&gt;

using namespace GClasses;
using std::cerr;
using std::cout;

int main(int argc, char *argv[])
{
	// Load the data
	GMatrix train;
	train.loadArff("/somepath/data/mnist/train.arff");
	GDataColSplitter sTrain(train, 1);
	GMatrix test;
	test.loadArff("/somepath/data/mnist/test.arff");
	GDataColSplitter sTest(test, 1);

	// Make the network
	GNeuralNet nn;
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 80));
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 30));
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, FLEXIBLE_SIZE));
	nn.setLearningRate(0.01);
	GAutoFilter af(&amp;nn, false);
	af.beginIncrementalLearning(sTrain.features(), sTrain.labels());

	// Prep the data
	af.prefilterData(&amp;sTrain.features(), &amp;sTrain.labels(), &amp;sTest.features(), &amp;sTest.labels());
	GMatrix&amp; trainFeatures = *af.data()[0];
	GMatrix&amp; trainLabels = *af.data()[1];
	GMatrix&amp; testFeatures = *af.data()[2];
	GMatrix&amp; testLabels = *af.data()[3];

	// Print some stats
	cout &lt;&lt; "% Training patterns: " &lt;&lt; to_str(trainFeatures.rows()) &lt;&lt; "\n";
	cout &lt;&lt; "% Testing patterns: " &lt;&lt; to_str(testFeatures.rows()) &lt;&lt; "\n";
	cout &lt;&lt; "% Topology: " &lt;&lt; to_str(nn.layer(0).inputs());
	for(size_t i = 0; i &lt; nn.layerCount(); i++)
		cout &lt;&lt; " -&gt; " + to_str(nn.layer(i).outputs());
	cout &lt;&lt; "\n";
	cout &lt;&lt; "% Total weights: " &lt;&lt; to_str(nn.countWeights()) &lt;&lt; "\n";

	// Train
	cout &lt;&lt; "@RELATION neural_net_training\n";
	cout &lt;&lt; "@ATTRIBUTE internal_rmse_train real\n";
	cout &lt;&lt; "@ATTRIBUTE internal_rmse_test real\n";
	cout &lt;&lt; "@ATTRIBUTE misclassification_rate real\n";
	cout &lt;&lt; "@DATA\n";
	GRandomIndexIterator ii(trainFeatures.rows(), nn.rand());
	for(size_t epoch = 0; epoch &lt; 10; epoch++)
	{
		// Validate
		double sseTrain = nn.sumSquaredError(trainFeatures, trainLabels);
		cout &lt;&lt; to_str(std::sqrt(sseTrain / trainFeatures.rows())) &lt;&lt; ", ";
		double sseTest = nn.sumSquaredError(testFeatures, testLabels);
		cout &lt;&lt; to_str(std::sqrt(sseTest / testFeatures.rows())) &lt;&lt; ", ";
		double mis = af.sumSquaredError(sTest.features(), sTest.labels());
		cout &lt;&lt; to_str((double)mis / sTest.features().rows()) &lt;&lt; "\n";
		cout.flush();

		// Do an epoch of training
		ii.reset();
		size_t index;
		while(ii.next(index))
			nn.trainIncremental(trainFeatures[index], trainLabels[index]);
	}
	return 0;
}
</pre>

<p>Here are the results that I get:</p>

<pre class="code">
% Training patterns: 60000
% Testing patterns: 10000
% Topology: 784 -> 80 -> 30 -> 10
% Total weights: 65540
@RELATION neural_net_training
@ATTRIBUTE internal_rmse_train real
@ATTRIBUTE internal_rmse_test real
@ATTRIBUTE misclassification_rate real
@DATA
1.0062320115962, 1.0062882869588, 0.9235
0.35827917057482, 0.35285602819834, 0.0588
0.29920038633219, 0.30503380076365, 0.0455
0.29897590956784, 0.30469283251474, 0.0436
0.27005888129929, 0.28255127487034, 0.0382
0.26209564354629, 0.28000584880966, 0.039
0.25383945740352, 0.27075626507427, 0.0361
0.23419057786692, 0.25915069511568, 0.0335
0.23568715771943, 0.26541501669647, 0.036
0.23326108174385, 0.26323053581819, 0.0332
</pre>

<p>The right-most column shows that we get 332 misclassifications after about 2 minutes of training.
You can get much better accuracy using bigger layers, but then training will take longer too.</p>

</p>Now, let's throw some more tricks onto the pile. Let's add a convolutional input layer, and two types of regularization: Dropout and Max-Norm.
Changes from the previous example are highlighted in red:</p>

<pre class="code">
#include &lt;iostream&gt;
#include &lt;cmath&gt;
#include &lt;GClasses/GApp.h&gt;
#include &lt;GClasses/GError.h&gt;
#include &lt;GClasses/GNeuralNet.h&gt;
#include &lt;GClasses/GActivation.h&gt;
#include &lt;GClasses/GTransform.h&gt;
#include &lt;GClasses/GVec.h&gt;
#include &lt;GClasses/GHolders.h&gt;

using namespace GClasses;
using std::cerr;
using std::cout;

int main(int argc, char *argv[])
{
	// Load the data
	GMatrix train;
	train.loadArff("/home/mike/data/mnist/train.arff");
	GDataColSplitter sTrain(train, 1);
	GMatrix test;
	test.loadArff("/home/mike/data/mnist/test.arff");
	GDataColSplitter sTest(test, 1);

	// Make the network
	GNeuralNet nn;
	<font color="red">nn.addLayer(new GLayerConvolutional2D(28, 28, 1, 7, 20, new GActivationBentIdentity()));
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 50, new GActivationBentIdentity()));
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 100, new GActivationBentIdentity()));</font>
	nn.addLayer(new GLayerClassic(FLEXIBLE_SIZE, FLEXIBLE_SIZE));
	nn.setLearningRate(0.01);
	GAutoFilter af(&amp;nn, false);
	af.beginIncrementalLearning(sTrain.features(), sTrain.labels());

	// Prep the data
	af.prefilterData(&amp;sTrain.features(), &amp;sTrain.labels(), &amp;sTest.features(), &amp;sTest.labels());
	GMatrix&amp; trainFeatures = *af.data()[0];
	GMatrix&amp; trainLabels = *af.data()[1];
	GMatrix&amp; testFeatures = *af.data()[2];
	GMatrix&amp; testLabels = *af.data()[3];

	// Print some stats
	cout &lt;&lt; "% Training patterns: " &lt;&lt; to_str(trainFeatures.rows()) &lt;&lt; "\n";
	cout &lt;&lt; "% Testing patterns: " &lt;&lt; to_str(testFeatures.rows()) &lt;&lt; "\n";
	cout &lt;&lt; "% Topology: " &lt;&lt; to_str(nn.layer(0).inputs());
	for(size_t i = 0; i &lt; nn.layerCount(); i++)
		cout &lt;&lt; " -&gt; " + to_str(nn.layer(i).outputs());
	cout &lt;&lt; "\n";
	cout &lt;&lt; "% Total weights: " &lt;&lt; to_str(nn.countWeights()) &lt;&lt; "\n";

	// Train
	cout &lt;&lt; "@RELATION neural_net_training\n";
	cout &lt;&lt; "@ATTRIBUTE internal_rmse_train real\n";
	cout &lt;&lt; "@ATTRIBUTE internal_rmse_test real\n";
	cout &lt;&lt; "@ATTRIBUTE misclassification_rate real\n";
	cout &lt;&lt; "@DATA\n";
	GRandomIndexIterator ii(trainFeatures.rows(), nn.rand());
	for(size_t epoch = 0; epoch &lt; 10; epoch++)
	{
		// Validate
		<font color="red">nn.scaleWeights(1.0 - 0.35, false, 1);</font>
		double sseTrain = nn.sumSquaredError(trainFeatures, trainLabels);
		cout &lt;&lt; to_str(std::sqrt(sseTrain / trainFeatures.rows())) &lt;&lt; ", ";
		double sseTest = nn.sumSquaredError(testFeatures, testLabels);
		cout &lt;&lt; to_str(std::sqrt(sseTest / testFeatures.rows())) &lt;&lt; ", ";
		double mis = af.sumSquaredError(sTest.features(), sTest.labels());
		cout &lt;&lt; to_str((double)mis / sTest.features().rows()) &lt;&lt; "\n";
		<font color="red">nn.scaleWeights(1.0 / (1.0 - 0.35), false, 1);</font>
		cout.flush();

		// Do an epoch of training
		ii.reset();
		size_t index;
		while(ii.next(index))
		<font color="red">{
			nn.maxNorm(0.0, 5.0, true);
			nn.trainIncrementalWithDropout(trainFeatures[index], trainLabels[index], 0.35);
		}</font>
	}
	return 0;
}
</pre>

<p>And here are some results. Notice they are not actually better. Is that because these tricks are no good? Is it
because this network has more weights and I didn't train for long enough? (Notice the accuracy is still improving.)
Is it because those tricks are really designed for larger networks, and this one is still so very small?
Is it because I didn't tune my hyperparameters at all? I will leave answering such questions as an exercize for
the reader--I'm just trying to show you how to use my code =).</p>

<pre class="code">
% Training patterns: 60000
% Testing patterns: 10000
% Topology: 784 -> 9680 -> 50 -> 100 -> 10
% Total weights: 491160
@RELATION neural_net_training
@ATTRIBUTE internal_rmse_train real
@ATTRIBUTE internal_rmse_test real
@ATTRIBUTE misclassification_rate real
@DATA
0.98167269423025, 0.98121969211094, 0.8799
0.50591353448385, 0.50227603143805, 0.0981
0.44455724838725, 0.44296446989617, 0.0805
0.43604353580815, 0.43670214066463, 0.0688
0.43654247522833, 0.43737355561063, 0.0634
0.44067857942438, 0.4451665813943, 0.0627
0.37784822100542, 0.38467632207642, 0.0529
0.47555594601116, 0.4806960384477, 0.0558
0.43522020483841, 0.44392465357931, 0.0537
0.34993765903977, 0.36091855594582, 0.0475
</pre>





<br><br>
<h3>Adaptive learning rate</h3>

<p>As an example, let's suppose you wanted to make a neural network with an adaptive learning rate.
Here's an example of how you could do something like that. It trains two neural networks,
one with a larger learning rate, and one with a smaller learning rate. At periodic intervals,
the learning rate is updated from the one that is doing better:</p>

<pre class="code">
class AdaptiveLearner
{
public:
	GNeuralNet nn1;
	GNeuralNet nn2;
	GAutoFilter af;
	double learningRate;
	const GMatrix&amp; features;
	const GMatrix&amp; labels;
	GRandomIndexIterator ii;


	AdaptiveLearner(const GMatrix&amp; feat, const GMatrix&amp; lab)
	: af(&amp;nn1, false), features(feat), labels(lab), ii(feat.rows(), nn1.rand())
	{
		learningRate = 1e-8;
		nn1.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 1500));
		nn1.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 1000));
		nn1.addLayer(new GLayerClassic(FLEXIBLE_SIZE, 500));
		nn1.addLayer(new GLayerClassic(FLEXIBLE_SIZE, FLEXIBLE_SIZE));
		af.beginIncrementalLearning(feat.relation(), lab.relation());
		nn2.copyStructure(&amp;nn1);
	}

	double validate(const GMatrix&amp; feat, const GMatrix&amp; lab)
	{
		return af.sumSquaredError(feat, lab) / lab.rows();
	}

	void trainEpoch()
	{
		ii.reset();
		size_t index;
		double sse1 = 0.0;
		double sse2 = 0.0;
		size_t counter = 0;
		nn1.setLearningRate(learningRate * 0.8);
		nn2.setLearningRate(learningRate * 1.25);
		while(ii.next(index))
		{
			const GVec&amp; in = af.prefilterFeatures(features[index]);
			const GVec&amp; targ = af.prefilterLabels(labels[index]);
			nn1.trainIncremental(in, targ);
			nn2.trainIncremental(in, targ);
			sse1 += GVec::squaredDistance(nn1.outputLayer().activation(), targ, nn1.outputLayer().outputs());
			sse2 += GVec::squaredDistance(nn2.outputLayer().activation(), targ, nn2.outputLayer().outputs());
			if(++counter &gt;= 2000)
			{
				// Dynamically tune the learning rate
				counter = 0;
				if(sse1 &lt; sse2)
				{
					nn1.copyWeights(&amp;nn2);
					learningRate = std::max(1e-8, learningRate * 0.8);
				}
				else if(sse2 &lt; sse1)
				{
					nn2.copyWeights(&amp;nn1);
					learningRate = std::min(0.1, learningRate * 1.25);
				}
				sse1 = 0.0;
				sse2 = 0.0;
			}
		}
	}
};
</pre>




<br>
<a href="bayesnet.html">Previous</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="bugs.html">Next</a>

<br><br><a href="../index.html#toc">Back to the table of contents</a><br>
</td></tr></table>
</td></tr></table><br><br><br><br><br>
</body></html>
